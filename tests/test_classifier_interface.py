from __future__ import annotations

import io
import os
import sys
import typing
from itertools import product
from typing import Callable, Literal

import numpy as np
import pandas as pd
import pytest
import sklearn.datasets
import torch
from sklearn import config_context
from sklearn.base import check_is_fitted
from sklearn.metrics import accuracy_score, log_loss
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.utils.estimator_checks import parametrize_with_checks
from torch import nn

from tabpfn import TabPFNClassifier
from tabpfn.preprocessing import PreprocessorConfig

from .utils import check_cpu_float16_support

devices = ["cpu"]
if torch.cuda.is_available():
    devices.append("cuda")

is_cpu_float16_supported = check_cpu_float16_support()

# TODO: test "batched" mode

feature_shift_decoders = ["shuffle", "rotate"]
multiclass_decoders = ["shuffle", "rotate"]
fit_modes = [
    "low_memory",
    "fit_preprocessors",
    "fit_with_cache",
]
inference_precision_methods = ["auto", "autocast", torch.float64, torch.float16]
remove_outliers_stds = [None, 12]
estimators = [1, 2]

all_combinations = list(
    product(
        estimators,
        devices,
        feature_shift_decoders,
        multiclass_decoders,
        fit_modes,
        inference_precision_methods,
        remove_outliers_stds,
    ),
)


@pytest.fixture(scope="module")
def X_y() -> tuple[np.ndarray, np.ndarray]:
    X, y = sklearn.datasets.load_iris(return_X_y=True)
    # Take 20 samples from class 0, 20 from class 1, 20 from class 2
    # This ensures all 3 classes are present
    X_diverse = np.vstack([X[y == 0][:20], X[y == 1][:20], X[y == 2][:20]])
    y_diverse = np.hstack([y[y == 0][:20], y[y == 1][:20], y[y == 2][:20]])

    # Shuffle to mix them up, otherwise training data would be ordered by class
    indices = np.arange(len(y_diverse))
    rng = np.random.default_rng(42)
    rng.shuffle(indices)

    return X_diverse[indices].astype(np.float32), y_diverse[indices].astype(np.int64)


class TestTabPFNClassifier:
    @pytest.mark.parametrize(
        (
            "n_estimators",
            "device",
            "feature_shift_decoder",
            "multiclass_decoder",
            "fit_mode",
            "inference_precision",
            "remove_outliers_std",
        ),
        all_combinations,
    )
    def test_fit(
        self,
        n_estimators: int,
        device: Literal["cuda", "cpu"],
        feature_shift_decoder: Literal["shuffle", "rotate"],
        multiclass_decoder: Literal["shuffle", "rotate"],
        fit_mode: Literal["low_memory", "fit_preprocessors", "fit_with_cache"],
        inference_precision: torch.types._dtype | Literal["autocast", "auto"],
        remove_outliers_std: int | None,
        X_y: tuple[np.ndarray, np.ndarray],
    ) -> None:
        if device == "cpu" and inference_precision in ["autocast"]:
            pytest.skip("CPU device does not support 'autocast' inference.")

        # Use the environment-aware check to skip only if necessary
        if (
            device == "cpu"
            and inference_precision == torch.float16
            and not is_cpu_float16_supported
        ):
            pytest.skip("CPU float16 matmul not supported in this PyTorch version.")

        model = TabPFNClassifier(
            n_estimators=n_estimators,
            device=device,
            fit_mode=fit_mode,
            inference_precision=inference_precision,
            inference_config={
                "OUTLIER_REMOVAL_STD": remove_outliers_std,
                "CLASS_SHIFT_METHOD": multiclass_decoder,
                "FEATURE_SHIFT_METHOD": feature_shift_decoder,
            },
            random_state=42,  # Added for consistency and reproducibility
        )

        X, y = X_y

        returned_model = model.fit(X, y)
        assert returned_model is model, "Returned model is not the same as the model"
        check_is_fitted(returned_model)

        probabilities = model.predict_proba(X)
        assert probabilities.shape == (
            X.shape[0],
            len(np.unique(y)),
        ), "Probabilities shape is incorrect"

        predictions = model.predict(X)
        assert predictions.shape == (X.shape[0],), "Predictions shape is incorrect!"

    @pytest.mark.parametrize(
        (
            "n_estimators",
            "device",
            "softmax_temperature",
            "average_before_softmax",
        ),
        list(
            product(
                [1, 4],  # n_estimators
                ["cpu", "cuda"] if torch.cuda.is_available() else ["cpu"],  # device
                [0.5, 1.0, 1.5],  # softmax_temperature
                [False, True],  # average_before_softmax
            )
        ),
    )
    def test_predict_logits_and_consistency(
        self,
        X_y: tuple[np.ndarray, np.ndarray],  # Changed to X_y
        n_estimators,
        device,
        softmax_temperature,
        average_before_softmax,
    ):
        """Tests the new predict_logits method and its consistency with predict_proba
        under various configuration permutations that affect the post-processing
        pipeline.
        """
        X, y = X_y  # Use X_y from fixture

        # Ensure y is float64 for consistency with original dataset type after slicing
        y = y.astype(np.int64)

        classifier = TabPFNClassifier(
            n_estimators=n_estimators,
            device=device,
            softmax_temperature=softmax_temperature,
            average_before_softmax=average_before_softmax,
            # Disable SKLEARN_16_DECIMAL_PRECISION for this test to avoid rounding
            # differences in predict_proba's internal output for comparison
            inference_config={"USE_SKLEARN_16_DECIMAL_PRECISION": False},
            random_state=42,  # Ensure reproducibility
        )
        classifier.fit(X, y)

        # 1. Test predict_logits output properties
        logits = classifier.predict_logits(X)
        assert isinstance(logits, np.ndarray)
        assert logits.shape == (X.shape[0], classifier.n_classes_)
        assert logits.dtype == np.float32
        assert not np.isnan(logits).any()
        assert not np.isinf(logits).any()
        if classifier.n_classes_ > 1:
            assert not np.all(logits == logits[:, 0:1]), (
                "Logits are identical across classes for all samples, indicating "
                "trivial output."
            )

        # 2. Test consistency: softmax(logits) should match predict_proba
        proba_from_predict_proba = classifier.predict_proba(X)

        # The relationship between predict_logits and predict_proba depends on the
        # averaging strategy.
        if n_estimators == 1 or average_before_softmax:
            # If there's only one estimator or we average before the softmax,
            # then applying softmax to the (already averaged) logits should
            # match the probabilities from predict_proba.
            proba_from_logits = torch.nn.functional.softmax(
                torch.from_numpy(logits), dim=-1
            ).numpy()
            np.testing.assert_allclose(
                proba_from_logits,
                proba_from_predict_proba,
                atol=1e-5,
                rtol=1e-5,
                err_msg=(
                    "Probabilities derived from predict_logits do not match "
                    "predict_proba output when they should be consistent."
                ),
            )
        else:
            # If n_estimators > 1 AND we average *after* softmax, then applying
            # softmax to the averaged logits will NOT match predict_proba.
            # predict_proba averages the probabilities, not the logits.
            # softmax(avg(logits)) != avg(softmax(logits))
            proba_from_logits = torch.nn.functional.softmax(
                torch.from_numpy(logits), dim=-1
            ).numpy()
            assert not np.allclose(
                proba_from_logits, proba_from_predict_proba, atol=1e-5, rtol=1e-5
            ), (
                "Outputs unexpectedly matched when averaging after softmax, "
                "indicating the logic path might be incorrect."
            )

        # 3. Quick check of predict  for completeness, derived from predict_proba
        predicted_labels = classifier.predict(X)
        assert predicted_labels.shape == (X.shape[0],)
        assert predicted_labels.dtype in [
            np.int64,
            object,
        ]

        # 4. Basic sanity check for predict and predict_proba outcomes
        assert accuracy_score(y, predicted_labels) >= 0.5
        assert log_loss(y, proba_from_predict_proba) < 5.0

    def test_softmax_temperature_impact_on_logits_magnitude(
        self,
        X_y: tuple[np.ndarray, np.ndarray],  # Changed to X_y
    ):
        """Ensures softmax_temperature impacts the magnitude of raw logits as
        expected: lower temperature -> higher magnitude (sharper distribution).
        """
        X, y = X_y  # Use X_y from fixture
        y = y.astype(np.int64)

        # Model with low temperature (should produce "sharper" logits)
        model_low_temp = TabPFNClassifier(
            softmax_temperature=0.1, n_estimators=1, device="cpu", random_state=42
        )
        model_low_temp.fit(X, y)
        logits_low_temp = model_low_temp.predict_logits(X)

        # Model with high temperature (should produce "smoother" logits)
        model_high_temp = TabPFNClassifier(
            softmax_temperature=10.0, n_estimators=1, device="cpu", random_state=42
        )
        model_high_temp.fit(X, y)
        logits_high_temp = model_high_temp.predict_logits(X)

        assert np.mean(np.abs(logits_low_temp)) > np.mean(
            np.abs(logits_high_temp)
        ), "Low softmax temperature did not result in more extreme logits."

        model_temp_one = TabPFNClassifier(
            softmax_temperature=1.0, n_estimators=1, device="cpu", random_state=42
        )
        model_temp_one.fit(X, y)
        logits_temp_one = model_temp_one.predict_logits(X)

        assert not np.allclose(
            logits_temp_one, logits_low_temp, atol=1e-6
        ), "Logits did not change with low temperature."
        assert not np.allclose(
            logits_temp_one, logits_high_temp, atol=1e-6
        ), "Logits did not change with high temperature."

    def test_balance_probabilities_alters_proba_output(
        self,
        X_y: tuple[np.ndarray, np.ndarray],  # Changed to X_y
    ):
        """Verifies that enabling `balance_probabilities` indeed changes the output
        probabilities (assuming non-uniform class counts).
        """
        X_full, y_full = X_y  # Use X_y from fixture

        # Introduce artificial imbalance to ensure balancing has an effect
        y_imbalanced = np.array(
            [0] * 30 + [1] * 5 + [2] * 5, dtype=np.int64
        )  # Total 40 samples

        # Create a subset of X to match the length of y_imbalanced
        X_subset = X_full[: len(y_imbalanced)]

        rng = np.random.default_rng(42)  # Initialize a new Generator with a seed
        rng.shuffle(y_imbalanced)

        # Model without class balancing
        model_no_balance = TabPFNClassifier(
            balance_probabilities=False, n_estimators=1, device="cpu", random_state=42
        )
        model_no_balance.fit(X_subset, y_imbalanced)
        proba_no_balance = model_no_balance.predict_proba(X_subset)

        # Model with class balancing enabled
        model_balance = TabPFNClassifier(
            balance_probabilities=True, n_estimators=1, device="cpu", random_state=42
        )
        model_balance.fit(X_subset, y_imbalanced)
        proba_balance = model_balance.predict_proba(X_subset)

        assert not np.allclose(
            proba_no_balance, proba_balance, atol=1e-5
        ), "Probabilities did not change when balance_probabilities was toggled."

    @parametrize_with_checks(
        [
            TabPFNClassifier(
                n_estimators=2,
                inference_config={"USE_SKLEARN_16_DECIMAL_PRECISION": True},
            ),
        ],
    )
    def test_sklearn_compatible_estimator(
        self,
        estimator: TabPFNClassifier,
        check: Callable[[TabPFNClassifier], None],
    ) -> None:
        if check.func.__name__ in (  # type: ignore
            "check_methods_subset_invariance",
            "check_methods_sample_order_invariance",
        ):
            estimator.inference_precision = torch.float64

        check(estimator)

    def test_balanced_probabilities(self, X_y: tuple[np.ndarray, np.ndarray]) -> None:
        """Test that balance_probabilities=True works correctly."""
        X, y = X_y

        model = TabPFNClassifier(
            balance_probabilities=True,
        )

        model.fit(X, y)
        probabilities = model.predict_proba(X)

        assert np.allclose(probabilities.sum(axis=1), 1.0)

        mean_probs = probabilities.mean(axis=0)
        expected_mean = 1.0 / len(np.unique(y))
        assert np.allclose(
            mean_probs,
            expected_mean,
            rtol=0.1,
        ), "Class probabilities are not properly balanced"

    def test_classifier_in_pipeline(self, X_y: tuple[np.ndarray, np.ndarray]) -> None:
        """Test that TabPFNClassifier works correctly within a sklearn pipeline."""
        X, y = X_y

        pipeline = Pipeline(
            [
                ("scaler", StandardScaler()),
                (
                    "classifier",
                    TabPFNClassifier(
                        n_estimators=2,  # Fewer estimators for faster testing
                    ),
                ),
            ],
        )

        pipeline.fit(X, y)
        probabilities = pipeline.predict_proba(X)

        assert np.allclose(probabilities.sum(axis=1), 1.0)

        mean_probs = probabilities.mean(axis=0)
        expected_mean = 1.0 / len(np.unique(y))
        assert np.allclose(
            mean_probs,
            expected_mean,
            rtol=0.1,
        ), "Class probabilities are not properly balanced in pipeline"

    def test_dict_vs_object_preprocessor_config(
        self, X_y: tuple[np.ndarray, np.ndarray]
    ) -> None:
        """Test that dict configs behave identically to PreprocessorConfig objects."""
        X, y = X_y

        dict_config = {
            "name": "quantile_uni_coarse",
            "append_original": False,  # changed from default
            "categorical_name": "ordinal_very_common_categories_shuffled",
            "global_transformer_name": "svd",
            "subsample_features": -1,
        }

        object_config = PreprocessorConfig(
            name="quantile_uni_coarse",
            append_original=False,  # changed from default
            categorical_name="ordinal_very_common_categories_shuffled",
            global_transformer_name="svd",
            subsample_features=-1,
        )

        model_dict = TabPFNClassifier(
            inference_config={"PREPROCESS_TRANSFORMS": [dict_config]},
            n_estimators=2,
        )

        model_obj = TabPFNClassifier(
            inference_config={"PREPROCESS_TRANSFORMS": [object_config]},
            n_estimators=2,
        )

        model_dict.fit(X, y)
        model_obj.fit(X, y)

        pred_dict = model_dict.predict(X)
        pred_obj = model_obj.predict(X)
        np.testing.assert_array_equal(pred_dict, pred_obj)

        prob_dict = model_dict.predict_proba(X)
        prob_obj = model_obj.predict_proba(X)
        np.testing.assert_array_almost_equal(prob_dict, prob_obj)

    class ModelWrapper(nn.Module):
        """Wrapper for the TabPFN model for ONNX export."""

        def __init__(self, original_model):  # noqa: D107
            super().__init__()
            self.model = original_model

        def forward(
            self,
            X,
            y,
            single_eval_pos,
            only_return_standard_out,
            categorical_inds,
        ):
            return self.model(
                None,
                X,
                y,
                single_eval_pos=single_eval_pos,
                only_return_standard_out=only_return_standard_out,
                categorical_inds=categorical_inds,
            )

    def _patch_layernorm_no_affine(self, model: nn.Module) -> None:
        """Workaround for ONNX export issue with LayerNorm(affine=False) in
        PyTorch <= 2.1.3.

        This patch function was necessary to enable successful ONNX export
        of the TabPFN model when using PyTorch version 2.1.3. The issue arose
        because the ONNX exporter in that version (and potentially earlier ones)
        failed to correctly handle `nn.LayerNorm` layers initialized with
        `affine=False`, which means they lack the learnable 'weight' (gamma) and
        'bias' (beta) parameters.

        However, testing indicated that this issue is resolved in later PyTorch
        versions; specifically, the ONNX export runs without errors on
        PyTorch 2.6.0 even without this patch.

        This function circumvents the problem by iterating through the model's
        modules and, for any `nn.LayerNorm` layer where `layer.weight` is None
        (indicating `affine=False`), it manually adds non-learnable
        (`requires_grad=False`) parameters for 'weight' (initialized to ones) and
        'bias' (initialized to zeros). This addition satisfies the requirements
        of the older ONNX exporter without changing the model's functional
        behavior, as these added parameters represent an identity affine
        transformation.
        """
        for layer in model.modules():
            if isinstance(layer, nn.LayerNorm) and layer.weight is None:
                # Build tensors on the same device/dtype as the layer's buffer
                device = next(layer.parameters(), torch.tensor([], device="cpu")).device
                dtype = getattr(layer, "weight_dtype", torch.float32)

                gamma = torch.ones(layer.normalized_shape, dtype=dtype, device=device)
                beta = torch.zeros_like(gamma)

                layer.weight = nn.Parameter(gamma, requires_grad=False)
                layer.bias = nn.Parameter(beta, requires_grad=False)

                # Optional: mark that we changed it (useful for logging)
                layer._patched_for_onnx = True

    @pytest.mark.filterwarnings("ignore::torch.jit.TracerWarning")
    def test_onnx_exportable_cpu(self, X_y: tuple[np.ndarray, np.ndarray]) -> None:
        if os.name == "nt":
            pytest.skip("onnx export is not tested on windows")
        if sys.version_info >= (3, 13):
            pytest.xfail("onnx is not yet supported on Python 3.13")
        X, y = X_y
        with torch.no_grad():
            classifier = TabPFNClassifier(n_estimators=1, device="cpu", random_state=42)
            # load the model so we can access it via classifier.model_
            classifier.fit(X, y)
            # this is necessary if cuda is available
            classifier.predict(X)
            # replicate the above call with random tensors of same shape
            X_tensor = torch.randn(
                (X.shape[0] * 2, 1, X.shape[1] + 1),
                generator=torch.Generator().manual_seed(42),
            )
            y_tensor = (
                torch.rand(y.shape, generator=torch.Generator().manual_seed(42))
                .round()
                .to(torch.float32)
            )
            dynamic_axes = {
                "X": {0: "num_datapoints", 1: "batch_size", 2: "num_features"},
                "y": {0: "num_labels"},
            }
            self._patch_layernorm_no_affine(classifier.model_)
            torch.onnx.export(
                self.ModelWrapper(classifier.model_).eval(),
                (X_tensor, y_tensor, y_tensor.shape[0], True, [[]]),
                io.BytesIO(),
                input_names=[
                    "X",
                    "y",
                    "single_eval_pos",
                    "only_return_standard_out",
                    "categorical_inds",
                ],
                output_names=["output"],
                opset_version=17,  # using 17 since we use torch>=2.1
                dynamic_axes=dynamic_axes,
            )

    @pytest.mark.parametrize("data_source", ["train", "test"])
    def test_get_embeddings(
        self, X_y: tuple[np.ndarray, np.ndarray], data_source: str
    ) -> None:
        """Test that get_embeddings returns valid embeddings for a fitted model."""
        X, y = X_y
        n_estimators = 3

        model = TabPFNClassifier(n_estimators=n_estimators, random_state=42)
        model.fit(X, y)

        # Cast to Literal type for mypy
        valid_data_source = typing.cast(Literal["train", "test"], data_source)
        embeddings = model.get_embeddings(X, valid_data_source)

        # Need to access the model through the executor
        model_instance = typing.cast(typing.Any, model.executor_).model
        encoder_shape = next(
            m.out_features
            for m in model_instance.encoder.modules()
            if isinstance(m, nn.Linear)
        )

        assert isinstance(embeddings, np.ndarray)
        assert embeddings.shape[0] == n_estimators
        assert embeddings.shape[1] == X.shape[0]
        assert embeddings.shape[2] == encoder_shape

    def test_pandas_output_config(self):
        """Test compatibility with sklearn's output configuration settings."""
        # Generate synthetic classification data
        X, y = sklearn.datasets.make_classification(
            n_samples=50,
            n_features=10,
            random_state=19,
        )

        # Initialize TabPFN
        model = TabPFNClassifier(n_estimators=1, random_state=42)

        # Get default predictions
        model.fit(X, y)
        default_pred = model.predict(X)
        default_proba = model.predict_proba(X)

        # Test with pandas output
        with config_context(transform_output="pandas"):
            model.fit(X, y)
            pandas_pred = model.predict(X)
            pandas_proba = model.predict_proba(X)
            np.testing.assert_array_equal(default_pred, pandas_pred)
            np.testing.assert_array_almost_equal(default_proba, pandas_proba)

        # Test with polars output
        with config_context(transform_output="polars"):
            model.fit(X, y)
            polars_pred = model.predict(X)
            polars_proba = model.predict_proba(X)
            np.testing.assert_array_equal(default_pred, polars_pred)
            np.testing.assert_array_almost_equal(default_proba, polars_proba)

    def test_constant_feature_handling(
        self, X_y: tuple[np.ndarray, np.ndarray]
    ) -> None:
        """Test that constant features are properly handled and
        don't affect predictions.
        """
        X, y = X_y

        # Create a TabPFNClassifier with fixed random state for reproducibility
        model = TabPFNClassifier(n_estimators=2, random_state=42)
        model.fit(X, y)

        # Get predictions on original data
        original_predictions = model.predict(X)
        original_probabilities = model.predict_proba(X)

        # Create a new dataset with added constant features
        X_with_constants = np.hstack(
            [
                X,
                np.zeros((X.shape[0], 3)),  # Add 3 constant zero features
                np.ones((X.shape[0], 2)),  # Add 2 constant one features
                np.full((X.shape[0], 1), 5.0),  # Add 1 constant with value 5.0
            ],
        )

        # Create and fit a new model with the same random state
        model_with_constants = TabPFNClassifier(n_estimators=2, random_state=42)
        model_with_constants.fit(X_with_constants, y)

        # Get predictions on data with constant features
        constant_predictions = model_with_constants.predict(X_with_constants)
        constant_probabilities = model_with_constants.predict_proba(X_with_constants)

        # Verify predictions are the same
        np.testing.assert_array_equal(
            original_predictions,
            constant_predictions,
            err_msg="Predictions changed after adding constant features",
        )

        # Verify probabilities are the same (within numerical precision)
        np.testing.assert_array_almost_equal(
            original_probabilities,
            constant_probabilities,
            decimal=5,
            err_msg="Prediction probabilities changed after adding constant features",
        )

    def test_classifier_with_text_and_na(self) -> None:
        """Test that TabPFNClassifier correctly handles text columns with NA values."""
        # Create a DataFrame with text and NA values
        # Create test data with text and NA values
        data = {
            "text_feature": [
                "good product",
                "bad service",
                None,
                "excellent",
                "average",
                None,
            ],
            "numeric_feature": [10, 5, 8, 15, 7, 12],
            "all_na_column": [
                None,
                None,
                None,
                None,
                None,
                None,
            ],  # Column with all NaNs
            "target": [1, 0, 1, 1, 0, 0],
        }

        # Create DataFrame
        df = pd.DataFrame(data)

        # Split into X and y
        X = df[["text_feature", "numeric_feature", "all_na_column"]]
        y = df["target"]

        # Initialize and fit TabPFN on data with text+NA and a column with all NAs
        classifier = TabPFNClassifier(device="cpu", n_estimators=2)

        # This should now work without raising errors
        classifier.fit(X, y)

        # Verify we can predict
        probabilities = classifier.predict_proba(X)
        predictions = classifier.predict(X)

        # Check output shapes
        assert probabilities.shape == (X.shape[0], len(np.unique(y)))
        assert predictions.shape == (X.shape[0],)
